### ===========================================
### LLM4CardGame - GLM4-9B LoRA SFT Config
### H100 96GB - 基于数据分析优化
### Token分布: P99=2928, Max=4865
### ===========================================

### Model
model_name_or_path: 'THUDM/glm-4-9b-chat'
dataset_dir: './data'

### Method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: all

### Dataset
dataset: card_sft
template: glm4
cutoff_len: 4096
max_samples: 1000000
overwrite_cache: true
preprocessing_num_workers: 16

### Output
output_dir: './output/sft_lora'
logging_steps: 10
save_strategy: steps
save_only_model: true
save_steps: 500
save_total_limit: 5
plot_loss: true
overwrite_output_dir: true

### Train - H100 96GB 优化配置
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

# disable_gradient_checkpointing: true

### 速度优化
dataloader_num_workers: 8
dataloader_pin_memory: true

### Eval
val_size: 0.01
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 500