# ### Model
# model_name_or_path: Qwen/Qwen2.5-7B-Instruct

# ### Method
# stage: sft
# do_train: true
# finetuning_type: lora
# lora_rank: 64
# lora_alpha: 128
# lora_dropout: 0.05
# lora_target: all

# ### Dataset
# dataset: cardgame_train
# dataset_dir: /data/jiacheng/system/cache/temp/icml2026/walking/LLM4CardGame/data
# template: qwen
# cutoff_len: 2048
# max_samples: 100000
# overwrite_cache: true
# preprocessing_num_workers: 16

# ### Output
# output_dir: /data/jiacheng/system/cache/temp/icml2026/walking/LLM4CardGame/output/sft_lora
# logging_steps: 10
# save_steps: 500
# plot_loss: true
# overwrite_output_dir: true

# ### Train - 针对H100 96GB优化
# per_device_train_batch_size: 32
# gradient_accumulation_steps: 4
# # 有效batch size = 32 × 4 = 128
# learning_rate: 2.0e-4
# num_train_epochs: 1.0
# lr_scheduler_type: cosine
# warmup_ratio: 0.1
# bf16: true
# ddp_timeout: 180000000

# # 速度优化
# dataloader_num_workers: 8
# dataloader_pin_memory: true
# torch_compile: false

# ### Eval
# val_size: 0.05
# per_device_eval_batch_size: 32
# eval_strategy: steps
# eval_steps: 500

### ===========================================
### LLM4CardGame - Optimized SFT Config
### 基于论文设置优化，适配H100 96GB
### ===========================================

### Model - GLM4-9B-Chat
model_name_or_path: THUDM/glm-4-9b-chat

### Method - 使用论文推荐的LoRA设置
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8           # 论文设置: 8 (不是64)
lora_alpha: 16         # 论文设置: 16 (不是128)
lora_dropout: 0.05
lora_target: all

### Dataset
dataset: card_sft
dataset_dir: /data/jiacheng/system/cache/temp/icml2026/walking/LLM4CardGame/data
template: glm4
cutoff_len: 4096       # 保持4096，斗地主prompt可能较长
max_samples: 1000000   # 100万样本，与论文一致
overwrite_cache: true
preprocessing_num_workers: 16

### Output
output_dir: /data/jiacheng/system/cache/temp/icml2026/walking/LLM4CardGame/output/sft_lora_glm4_v2
logging_steps: 10
save_steps: 500
save_total_limit: 5    # 只保留最近5个checkpoint
plot_loss: true
overwrite_output_dir: true

### Train - H100 96GB优化
per_device_train_batch_size: 64    # H100可支持更大batch
gradient_accumulation_steps: 2     # 有效batch = 64 × 2 = 128 (论文设置)
learning_rate: 1.0e-4              # 论文设置
num_train_epochs: 1.0              # 1个epoch足够
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

# 速度优化
dataloader_num_workers: 8
dataloader_pin_memory: true

### Eval
val_size: 0.01         # 1%验证集 = 10000样本
per_device_eval_batch_size: 64
eval_strategy: steps
eval_steps: 500

### ===========================================
### 预计训练时间: 8-12小时 (vs 原来的27天)
### 预计步数: 1,000,000 / 128 ≈ 7,813步
### 预计Win Rate: 0.75-0.80 (接近论文水平)
### ===========================================